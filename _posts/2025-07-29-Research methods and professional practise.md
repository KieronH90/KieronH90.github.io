---
layout: post
title: Research Methods and Professional Practice
subtitle: A reflective journey
categories: Research
tags: [Research, professional practice]
---

## Task 1 - Ethics in Computing in the age of Generative AI 

The rapid emergence of generative AI from late 2022 has profoundly impacted technology and society, with Computer Science at its core. While AI itself isn't new, this generative iteration demands a re-evaluation of what is normal and the urgent establishment of new rules in order to prevent misuse.
Correa et al. (2023) rightly highlight the critical need to define guiding values for AI. Their observation that "a key challenge... lies in establishing a consensus on these values, given the diverse perspectives of various stakeholders worldwide and the abstraction of normative discourse" accurately reflects the current global AI governance landscape. Countries like the EU, US, and China are adopting varied approaches, underscoring the complexity of unified global frameworks (International Actuarial Association, 2025; Times of India, 2025).

Deckard (2023) notes how generative AI challenges traditional views of tasks requiring creativity and social intelligence, impacting employment and human-AI collaboration. Lee et al. (2025) further suggest that generative AI can reduce cognitive effort and foster over-reliance, potentially diminishing critical thinking. These developments deepen ethical and regulatory complexities as human and machine outputs increasingly blur.
My view is that a fragmented approach to generative AI governance, while understandable, poses significant long-term risks that could lead to unforeseen outcomes. Without global coordination, we risk stifling responsible innovation in some regions while allowing unchecked development in others. Universal challenges like bias, privacy, intellectual property, misinformation, and accountability are not confined by borders (KorumLegal, 2024; Coursera, 2025). Therefore, an effective and ethical course of action must be built on international cooperation and a shared commitment to responsible AI.
The best approach, in my opinion, would be a multi-pronged one. One which prioritises international collaboration on foundational principles while allowing for national and regional specificities.

A. Establish a Global AI Governance Body: Given AI's global impact, a dedicated international body, possibly a new or expanded UN agency, would be crucial. This body would not impose rigid laws but facilitate dialogue, consensus-building, and the development of voluntary, high-level principles and best practices, acting as an intermediary or mediator of discussion between authorities and global powers. The Royal Society (2024) emphasizes the UN's potential for creating a common vocabulary and coordinating minimum standards. Its functions would include:
•	Harmonising ethical guidelines: Building on efforts like UNESCO's, this body would identify common ethical principles (e.g., fairness, transparency, accountability, human oversight) across diverse cultures. Diya (2023) suggests universal human rights principles could guide consensus. Correa et al. (2023) note "transparency," "explainability," "reliability," and "fairness" as globally cited principles.
•	Developing interoperable standards: Work towards technical and interoperability standards for AI systems, particularly for data privacy, security, and explainability. This would facilitate cross-border data flows and ensure a baseline of safety.
•	Knowledge sharing and capacity building: Serve as a hub for research, best practices, and regulatory experiences, supporting developing nations in building AI governance capabilities.
•	Monitoring and incident reporting: Create mechanisms for tracking global AI impact, identifying risks, and coordinating responses to AI-related incidents (e.g., misinformation).
Impact on Legal, Social, and Professional Issues:
•	Legal Issues: This approach would provide a strong normative foundation for national legislation, encouraging legal convergence over time. It would reduce fragmentation, foster legal certainty for international businesses, and inform future international treaties. Addressing intellectual property and accountability globally would lead to more consistent legal precedents.
•	Social Issues: A globally coordinated approach would mitigate harmful AI applications like discriminatory systems or misinformation amplification. Promoting inclusive debates would foster a more equitable distribution of AI's benefits and address power concentration concerns (Michigan Online, 2025). This could also lead to a greater emphasis on societal well-being in AI development.
•	Professional Issues: For computing professionals, this framework would provide clearer ethical guidelines and professional standards, reducing ambiguity and supporting responsible innovation. It would empower professionals to advocate for ethical AI within their organisations, backed by international principles. It could also drive demand for interdisciplinary skills. The BCS (2024) highlights the importance of professional bodies in supporting members facing ethical challenges and advocating for stronger standards. This global framework would bolster such initiatives.

B. National and Regional Implementation with Contextual Adaptability: While global principles are vital, specific regulatory frameworks must be tailored to national contexts.
•	Risk-based regulation: Countries should adopt a risk-based approach, like the EU AI Act, with stricter regulations for high-risk AI applications (e.g., critical infrastructure, healthcare, law enforcement).
•	Public-private partnerships: Governments should collaborate with industry, academia, and civil society to develop practical guidelines and self-regulatory mechanisms, ensuring technical feasibility and responsiveness to rapid technological advancements.
•	Education and public awareness: Invest in public education to increase AI literacy and promote informed discourse about its benefits and risks.
In conclusion, the generative AI revolution offers an opportunity for a new paradigm in global governance. A collaborative, multi-tiered approach, starting with UN-facilitated consensus on high-level principles and cascading into context-specific national regulations, is the most suitable course of action. This framework would safeguard against risks, foster responsible innovation, and ensure that generative AI serves humanity's collective good, addressing the complex legal, social, and professional issues faced by computing professionals.

*References*

BCS, The Chartered Institute for IT. (2024) Living with AI and emerging technologies: Meeting ethical challenges through professional standards. Available at: https://www.bcs.org/articles-opinion-and-research/living-with-ai-and-emerging-technologies-meeting-ethical-challenges-through-professional-standards/ (Accessed: 29 July 2025).
Correa, D., et al. (2023) ‘AI Governance: A Systematic Literature Review’, ResearchGate. Available at: https://www.researchgate.net/publication/382523579_AI_Governance_A_Systematic_Literature_Review (Accessed: 29 July 2025).
Deckard. (2023) Generative AI and the Future of Work: A Reappraisal Working Paper No. 2023. Available at: https://oms-www.files.svdcdn.com/production/downloads/academic/2023-FoW-Working-Paper-Generative-AI-and-the-Future-of-Work-A-Reappraisal-combined.pdf (Accessed: 29 July 2025).
Diya, A. (2023) Applying International Human Rights Principles for AI Governance. CIGI Online. Available at: https://www.cigionline.org/documents/3154/no.196_Diya_clNidFE.pdf (Accessed: 29 July 2025).
Lee, K., Han, J., & Lee, S. (2025) The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers. Microsoft. Available at: https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf (Accessed: 29 July 2025).
The Royal Society. (2024) The United Nations' role in international AI governance. Available at: https://royalsociety.org/-/media/policy/publications/2024/un-role-in-international-ai-governance.pdf (Accessed: 29 July 2025).

## Task 2 - Collaborative Learning Discussion

In this task, we were asked to analyse an ACM ethics case, applying ACM/BCS codes, and discussing legal, social, and professional impacts. We chose Corazón's medical implant case, scrutinising its 'negligible risk' assessment of a vulnerability. Our analysis highlighted the critical need for integrity and patient safety in computing professionalism. Below is a screenshot of my initial post:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussioninitialpost.png)

Next, we were required to respond to 3 of the posts that had been written by our peers. Below are each of the posts, followed by my response:

*Julius*
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/CollaborativediscussionJulispost.png)

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/CollaborativediscussionJulisresponse.png)

*Shraddha*
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/CollaborativediscussionShraddhapost.png)

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/CollaborativediscussionShraddharesponse.png)

*Valentina*
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/CollaborativediscussionValentinapost.png)

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/CollaborativediscussionValentinaresponse.png)

Finally, I was asked to write a post summarising the discussions held and to reflect on how this impacted upon my professional opinions:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussionsummarypost.png)

## Task 3 - Literature Review Overview

In this task, we were asked to create an outline for the approach we plan to take when writing our literature review later in the module. The plan and structure that I formulated is as follows:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/literatureoverviewpage1.png)
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/literatureoverviewpage2.png)
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/literatureoverviewpage3.png)

This approach was accepted with overwhelmingly positive feedback, with some crucial small changes to be made in order to improve upon my final submission. 

## Task 4 -Inappropriate Use of Surveys

This entry examines the unethical use of surveys, where data collection is presented under false pretences. The Cambridge Analytica (CA) scandal is the primary case study, followed by other deceptive practices.

1. Case Study: Cambridge Analytica (CA)
The CA scandal revealed how a seemingly harmless Facebook survey app, "thisisyourdigitallife," was used for unethical data harvesting (Confessore, 2018).

How it Happened: A researcher, Aleksandr Kogan, paid a few hundred thousand users to take a personality quiz. In doing so, Facebook's API allowed his app to also harvest the personal data of their entire friend networks—tens of millions of people who never consented. Kogan then violated Facebook's policies by selling this data to Cambridge Analytica.

Why it was used: CA used the data to build detailed psychographic profiles of voters. These profiles were then used to create and target highly personalised, manipulative political advertising during major political events like the 2016 US election and Brexit.

2. Further Examples of Survey Misuse
Push Polling: This is a political campaign tactic masquerading as a poll. An operative calls a voter and asks leading or slanderous questions ("Would you still vote for Candidate X if you knew they were being investigated for corruption?"). The goal is not to collect data, but to plant negative information and influence the vote.

'Sugging' (Selling Under the Guise of Research): This is a deceptive sales technique. A "researcher" asks survey-like questions to qualify a person as a sales lead ("Do you travel often?"). Once qualified, the interaction pivots to a sales pitch ("Since you travel, you'd be perfect for our new credit card.").

3. Ethical, Social, Legal, and Professional Impacts
All three examples are built on a core deception, leading to severe, overlapping consequences.

Cambridge Analytica: This was a catastrophic failure of informed consent and a direct violation of the ACM code to "Avoid Harm" (ACM, 2018). It caused a massive erosion of public trust in technology and democratic processes. Legally, it resulted in a $5 billion FTC fine for Facebook (FTC, 2019) and accelerated data privacy laws like the GDPR. For the professionals involved, it was a complete breach of ethics, using their skills for societal harm.

Push Polling: This practice is fundamentally dishonest. It pollutes the public discourse with misinformation, increases voter cynicism, and damages the credibility of legitimate polling. It is banned by all professional research bodies as unethical.

'Sugging': This is a deceptive bait-and-switch that exploits public trust. Its main social harm is creating "survey fatigue," which makes the public unwilling to participate in legitimate academic or government research. It is universally condemned and prohibited by market research codes of conduct (MRS, n.d.).

References
ACM (2018) ACM Code of Ethics and Professional Conduct. Available at: https://www.acm.org/code-of-ethics (Accessed: 18 October 2025).

Confessore, N. (2018) 'Cambridge Analytica and Facebook: The Scandal and the Fallout, Explained', The New York Times, 4 April. Available at: https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html (Accessed: 18 October 2025).

FTC (2019) FTC Imposes $5 Billion Penalty and Sweeping New Privacy Restrictions on Facebook. Available at: https://www.ftc.gov/news-events/news/press-releases/2019/07/ftc-imposes-5-billion-penalty-sweeping-new-privacy-restrictions-facebook (Accessed: 18 October 2025).

MRS (n.d.) Sugging & Frugging. Available at: https://www.mrs.org.uk/research/sugging-frugging (Accessed: 18 October 2025).

## Task 5 -Critical analysis of Surveys

For this task, I was asked to critically analyse a questionnaire of my choice. I selected this health questionnaire from the Oxford University Hospital NHS Trust (a snippet is shown below)

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Questionnaireanalysissnippet.png)

This document is a pre-operative health screening questionnaire designed to assess patient risk and accessibility needs. While comprehensive in its topics, I find its format and question design are flawed. These issues could lead to patient confusion, inaccurate data, and potentially missed clinical risks.

The format is the most significant weakness. The table layout visually disconnects questions from their "Yes/No" checkboxes, which are on the far right. This increases cognitive load and the risk of error. Furthermore, the "Click here to add comment" fields are a clunky user interface choice, far less intuitive than a simple web form.

The question wording is also problematic.


Ambiguity: Q11 ("Problematic wheezing") and Q13 ("significant infections") use subjective words that require a patient to make a clinical judgment they are unqualified for. This is a common flaw that can damage data validity (Rattray and Jones, 2007).

Contradiction: Q5 (blood pressure) is contradictory. It asks about "Uncontrolled" high blood pressure but then instructs patients who take medication (and may be well-controlled) to tick "Yes," making the resulting data unreliable.


Redundancy: Q25 ("More than 3 prescribed medicines?") is redundant, as Q30 asks for a complete list.

In contrast, Q31 (Functional capacity) is an excellent question. It uses a validated scale of everyday activities to estimate a patient's Metabolic Equivalents (METS), which is a strong predictor of perioperative risk (Mohan et al., 2021).

Recommendations for Improvement
I believe the questionnaire could be vastly improved with three key changes:

Re-format: Convert this from a Word document to a single-column web form. Place "Yes/No" options immediately adjacent to their question.

Rewrite Ambiguous Questions: Replace subjective words with objective questions.

Rewrite Q5: Split it into "5a. Have you been diagnosed with high blood pressure?" and "5b. Do you take medication for it?"

Rewrite Q11: Change to "Have you been diagnosed with asthma or COPD?"

Streamline Sections: Remove redundant questions like Q25. Move the "Please list all... medications" (Q30) to the top of the medication section to serve as the primary source of information.

References
Mohan, R., D'Souza, R. and Abd-Elsayed, A. (2021) 'Functional Capacity Evaluation in Preoperative Assessment', Current Pain and Headache Reports, 25(9), p. 58.

Rattray, J. and Jones, M.C. (2007) 'Essential elements of questionnaire design and development', Journal of Clinical Nursing, 16(2), pp. 234-243.

## Task 6 - Hypothesis testing worksheet

During this task, I was asked to work through a series of instructions, the output of which are shown below:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Unit71.png)
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Unit72.png)

Next I was provided with a new dataset, and asked to analyse this new data using what I had learned. The output from said dataset is below:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Unit73.png)

"p2" Value
Value: 0.43649248

Interpretation: This value does not relate to the t-test for means. Instead, p2 is the two-tailed p-value for the F-Test for Variances.

Analysis: This value is used to decide which t-test to run (Equal vs. Unequal variances). Since 0.436>0.05, it confirms there is no significant difference between the variances of the two groups. This validates the choice to use the "t-Test: Two-Sample Assuming Equal Variances".

I cannot use p2 to determine if the mean incomes are different.

Difference in Means
Value: 8.68

Interpretation: This is the straightforward arithmetic difference between the sample means of Variable 1 (Males) and Variable 2 (Females). On average, the males in the sample earned 8.68 units more than the females (52.91−44.23=8.68).

Significance: This value tells us the size of the effect, but not if it's statistically significant.


Combined Interpretation
To get the full picture, we must combine the "Difference in Means" with the correct p-value from the t-test:

The average income for males in the sample was 8.68 units higher than for females.

The one-tailed p-value for this difference is 0.0007.

Conclusion: Because p<0.05, the observed difference of 8.68 is statistically significant. We can conclude that the population mean income for males is greater than that of females.

Next, I was asked to perform statistical analysis on the data below:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Unit74.png)

The P-value Method
My Result: The p-value is P(T<=t) two-tail = 0.007545995.

Analysis: This p-value (p=0.0075) is the probability of observing a difference this large (or larger) between the two agents if there were actually no difference.

Decision: Since 0.0075<0.05, my result is statistically significant. We reject the null hypothesis (H0).

After this I was asked to discuss the implications of changing this 2 tailed test to a 1 tailed test. 

1. My Hypotheses
First, I needed to define my hypotheses. The question asks if Filter Agent 1 was the more effective. Since "more effective" means it results in less impurity, I am testing if the population mean for Agent 1 is lower than for Agent 2.

Null Hypothesis (H0): The mean impurity for Agent 1 is not lower than for Agent 2 (μ1≥μ2).

Alternative Hypothesis (H1): The mean impurity for Agent 1 is lower than for Agent 2 (μ1<μ2).

2. The Relevant P-value
For this one-tailed test, I looked at the one-tailed p-value in the Excel output:

P(T<=t) one-tail = 0.003772997 (which I'll round to p=0.0038)

3. My Conclusion
Using my standard significance level of α=0.05, I compared my p-value to this alpha.

Since p=0.0038 is less than α=0.05, my results are statistically significant.

Therefore, I reject the null hypothesis (H0). Based on this analysis, I can conclude that there is strong evidence that Filter Agent 1 is the more effective filtration agent, as it results in a significantly lower population mean impurity level than Filter Agent 2.

## Task 7 - Case Study: Accuracy of information

In this task I was asked to discuss an ethical dilemma, relating to a statistical programmer named Abi whom works for a fictional cereal named Whizz. She has detected that some company claims appear not to be true. Below is my initial post on the subject.

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2initialpost.png)

Next are responses from 3 of my peers:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2Valentinaresponse.png)
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2Shashankresponse.png)
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2Guilhermeresponse.png)

And 3 posts written by my peers and my responses:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2initialpostJulius.png)
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2initialpostJuliusresponse.png)

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2initialpostMd.png)
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2initialpostMdresponse.png)

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2initialpostValentina.png)
![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2initialpostValentinaresponse.png)

Finally, here is my summary post:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Collaborativediscussion2summarypost.png)

## Task 8 - Bar Charts in Excel

In this task, I was provided with a dataset, comparing the brand preferences in 2 different areas. I was then asked to visually represent this data, and draw comparisons. The visualisation is below:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Unit91.png)

Overall, Area 2 shows a much stronger preference for Brands A and B than Area 1. In both regions, "Other" brands are the most popular, but their dominance is far greater in Area 1.

Key Observations
Brand Preference in Area 1:
"Other" is the clear winner, preferred by a 60.0% majority.
Brand B is the second choice at 24.3%.
Brand A is last at 15.7%.

Brand Preference in Area 2:
"Other" is still the top choice, but by a much smaller margin at 45.6%.
Brand B is significantly more popular here at 33.3%.
Brand A also has a stronger showing at 21.1%.

Summary
Market Strength: Area 2 is a much better market for both named brands. Brand B's preference is ~9 points higher, and Brand A's is ~5 points higher.
Brand Rank: The order of preference is the same in both regions: 1st: Other, 2nd: Brand B, 3rd: Brand A.
Combined Share: In Area 1, Brands A and B combined only have a 40% share. In Area 2, they command a 54.4% majority share, making them much more competitive against the "Other" brands.

In the next task, I was asked to analyse the sparsity and abundance of a particular type of heather in 2 different regions. The visualised data is below:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Unit92.png)

Location A (Blue Bars): This area is dominated by heather
The most common classification is Abundant (approx. 46%).
Combined, 85% of the areas in Location A have heather (roughly 46% Abundant + 39% Sparse).
It is very rare for heather to be Absent (only about 14%).

Location B (Red Bars): This area has very little heather.
The single largest category by far is Absent (approx. 45%).
The remaining areas are split, with "Sparse" (32%) being more common than "Abundant" (23%).

Summary
The contrast between the two is most striking at the extremes:
You are twice as likely to find "Abundant" heather in Location A as you are in Location B (46% vs. 23%).
You are three times as likely to find "Absent" heather in Location B as you are in Location A (45% vs. 14%).

In the following task, I was asked to analyse the performance of 2 different diets using a histogam as shown below:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Unit93.png)

1. Overall Effectiveness (The "Hump")
Diet A: The vast majority of participants (around 80%) are clustered in the 3 kg to 9 kg weight loss range. The most common results (the "peak") are between 5 and 9 kg, where 60% of all users landed.
Diet B: The results are clustered at a much lower weight loss. The vast majority (around 72%) are in the 1.5 kg to 5.5 kg range. The peak for this diet is clearly lower than Diet A.

2. Risk (Weight Gain / Low Loss)
Diet A: A very small portion of users (around 2%) were in the -1 to 1 kg bin, meaning they gained or didn't lose weight.
Diet B: This diet had a higher risk of failure. A larger group (around 6%) fell into the -2 to 1.5 kg bin.

3. High Achievers (The "Tail")
Diet A: This diet had a significant group of "high achievers." About 10% of its users lost between 9 and 11 kg.
Diet B: High achievers were rare. Only about 2% of its users lost a large amount of weight (9.5 to 12.5 kg).

Summary
In short, Diet A was better in almost every way:
Its typical user lost more weight (5-9 kg range vs. 1.5-5.5 kg).
It had more high achievers (10% vs. 2%).
It had a lower risk of users gaining weight (2% vs. 6%).

## Task 9 - Practicing Business Visualisation with PowerBI

In this unit I was tasked with completing a course on Design Power BI reports. As shown in the completion statement below:

![My logo](/assets/images/ResearchMethodsandProfessionalPractice/Unit94.png)

## Literature review evaluation

The following feedback was received based on my literature review:

"1. Knowledge and Understanding

This literature review presents a comprehensive, well-contextualised, and technically nuanced understanding of the application of AI in secondary mathematics education in the UK. The author demonstrates an exceptional grasp of both pedagogical and technological domains, engaging critically with the ways in which machine learning and adaptive systems reshape the learning experience. The specificity to mathematics, a subject requiring both procedural fluency and conceptual depth, allows the discussion to remain focused yet insightful. The use of a clear conceptual framework and well-defined research question enhances clarity and purpose throughout.

2. Criticality

Critical engagement is a major strength of this review. The author systematically evaluates both the benefits and challenges of AI-driven platforms, moving beyond surface-level claims to interrogate issues such as algorithmic bias, the digital divide, and the evolving role of the teacher. The treatment of socio-technical tensions—particularly around data ethics, algorithm fairness, and long-term engagement—demonstrates analytical maturity. The critique is grounded in theory (e.g., engagement models, constructivist perspectives) and complemented by empirical and policy evidence. The conclusion meaningfully synthesises literature gaps and practical implications, indicating strong evaluative judgement.

3. Use of Relevant Sources

The review draws on an extensive and well-selected range of sources, including peer-reviewed studies, government policy documents, and foundational theoretical works. The citation of literature across AI, education, learning science, and ethics ensures interdisciplinary rigour. Sources are recent (predominantly from the last 5–7 years), relevant, and well-integrated into the narrative. The author engages critically with both supportive and dissenting views, particularly when discussing the complexities of measuring attainment and engagement. Referencing is consistent and adheres to Harvard style.

4. Structure and Presentation

The review is excellently structured. The use of thematic sections aligned with the research question supports clear argument progression. The inclusion of sections such as “Conceptual Framework,” “Engagement,” “Attainment,” and “Pedagogical and Technological Factors” reflects strong organisational logic. Transitions are smooth, and each section builds upon the previous. Language is precise, academic, and well suited to a postgraduate audience. Paragraphing is well managed, and the overall presentation is polished and professional. The inclusion of clearly framed implications and future research directions strengthens the scholarly contribution.

5. Academic Integrity

There is full evidence of academic integrity. Sources are appropriately cited throughout, with a balanced use of paraphrasing, quotation, and synthesis. There are no signs of plagiarism or citation lapses. The reference list is complete, accurate, and correctly formatted in line with Harvard conventions. The author demonstrates full adherence to academic standards."

- Distinction, Dr Diego Navarra 

I am extremely pleased with this highly positive and affirming feedback. It not only confirms the effectiveness of my work but also validates the specific methodological and analytical choices I made during the research and writing process.

The feedback on "Knowledge and Understanding" and "Structure" (Points 1 and 4) is particularly encouraging. I made a conscious decision to narrow the scope to "UK secondary mathematics" and to build the entire review around a "clear conceptual framework." The tutor’s comments confirm that this strategy was successful, allowing for a "technically nuanced" and "focused" discussion (Point 1) that was supported by "strong organisational logic" and "thematic sections" (Point 4). This affirms my belief that a robust structure is essential for making complex arguments clear and purposeful.

The most significant feedback for me is on "Criticality" (Point 2). This is the area I focused on most, aiming to move beyond a simple description of sources. I am delighted that the tutor recognised my efforts to "systematically evaluate" and "interrogate issues" like algorithmic bias and the digital divide. The comment on "analytical maturity" (Point 2) gives me great confidence, as it confirms my ability to ground practical problems in theoretical frameworks. This was directly supported by the "extensive and well-selected range of sources" (Point 3), where I deliberately sought out interdisciplinary and "dissenting views" to provide the raw material for a balanced critique.

This feedback doesn't highlight any weaknesses, so my main takeaway is the need to maintain this high standard. It provides a clear blueprint for my final dissertation. The "literature gaps and practical implications" (Point 2) that I identified will now serve as the direct foundation for my own research question and methodology. The challenge will be to apply this same level of critical, systematic, and ethically-aware analysis to my own primary data collection and to ensure my final write-up is just as "polished and professional" (Point 4).

Overall, this feedback is a significant motivator and assures me that my research is on a solid and sophisticated footing.

## Research Proposal Review

The following feedback was received based on my research proposal:

"Knowledge and Understanding (25%)

You show strong awareness of the current AI in education landscape, clearly framing the gap between the promise of adaptive, personalised learning and the practical realities of implementation. The use of machine learning, data analytics, and adaptive platforms is well contextualised, and you ground the project in UK secondary maths education—a clear, well-justified domain.

Criticality (25%)

The proposal shows thoughtful critique, noting algorithmic bias, data privacy, and socio-economic inequality. You also stress that teachers remain “orchestrators of learning,” which is an insightful pedagogical perspective. To further strengthen criticality, you could interrogate how bias detection will be operationalised (e.g., fairness metrics), and critically compare different adaptive AI approaches beyond the broad ITS/ALP categories.

Presentation & Communication Skills (30%)

The transcript is clear, professional, and engaging, even with some natural hesitations due to illness. The oral narrative flows logically and explains complex ideas in accessible terms. The slides are visually simple and well aligned with narration, but some (e.g., “Promise vs Reality”) could use more visual aids or data-driven infographics to keep the audience engaged.

Structure & Presentation (10%)

The presentation follows the required academic structure very well: problem framing, literature, RQ, aims, methodology, ethics, artefact, and timeline are all included and logically sequenced. The 10-month timeline is realistic and shows good planning.

Use of Sources (10%)

You draw on relevant, credible literature, including both foundational works (O’Neil, Selwyn) and contemporary studies. Some references could be expanded to include very recent AI in education policy or UK EdTech adoption reports to increase currency."

- Merit, Dr Diego Navarra

This feedback is extremely helpful and provides a clear, actionable roadmap for developing my proposal into a final dissertation. I am pleased that the core of my project is considered strong, but I particularly value the specific, constructive criticism which highlights clear areas for improvement.

1. What Went Well
I am very encouraged by the positive feedback on "Knowledge and Understanding" and "Structure & Presentation". This confirms that my fundamental concept—the gap between AI's promise and its practical reality—is a strong, well-justified foundation for the project. The tutor's validation of my "insightful pedagogical perspective" (from the "Criticality" section) on teachers as "orchestrators of learning" gives me confidence that I am successfully blending educational theory with technology.

I am also grateful for the positive comments on my "Presentation & Communication". I was concerned that my illness would detract from the delivery, so I am relieved that the tutor found it "clear, professional, and engaging" despite this.

2. What Needs Improvement
The feedback has identified three key areas where I need to add more depth.

Deepening Criticality: My current critique is (as the tutor noted) "thoughtful" but too general. The suggestion to move from identifying bias to operationalising its detection is the most important takeaway.
Action: I will research specific "fairness metrics" used in machine learning to provide a concrete, technical dimension to my critique. I will also expand my literature review to "critically compare" specific adaptive AI approaches (e.g., Bayesian Knowledge Tracing vs. a neural network approach) instead of just using broad categories like ITS.

Enhancing Visual Engagement: The feedback that my slides were "visually simple" but could be more engaging is a fair point. I prioritised clarity over impact.
Action: For future presentations, I will incorporate "data-driven infographics" as suggested. For example, my "Promise vs. Reality" slide would be far more effective with a chart showing actual EdTech adoption rates or student attainment data, rather than just text.

Updating Sources: The point on "Use of Sources" is a straightforward and excellent recommendation.
Action: My foundational literature is strong, but I will now actively search for the very latest (2024-2025) UK government AI/EdTech policy documents and industry adoption reports to ensure my project has maximum "currency."

Conclusion
Overall, this feedback provides the perfect balance of validation and challenge. I feel confident in my project's core structure and research gap. My clear priorities for the next stage are to sharpen my technical critique, improve my data visualisation skills, and update my sources to be as current as possible.
